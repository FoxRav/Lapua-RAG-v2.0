Johtopäätös
Vaihdetaan Lapua RAG -backendin LLM-kerros käyttämään **Groq Cloudin mallia `openai/gpt-oss-120b`**. RAG-pipeline (Docling → Qdrant) pysyy samana; vaihdetaan vain paikka, jossa vastaus generoidaan. Alla valmis ohjeistus, jonka voit syöttää Cursorille vaihe kerrallaan.

---

### Toimi näin (max 5 kohtaa)

**1. Lisää Groq-riippuvuus ja ympäristömuuttuja**

Pyydä Cursorilta esim.:

> Päivitä `apps/backend/pyproject.toml` (tai `requirements.txt`) lisäämällä Groq-client:
>
> ```toml
> [project]
> dependencies = [
>   "fastapi",
>   "uvicorn",
>   "groq>=0.11.0",
>   # muut...
> ]
> ```
>
> Tee myös `apps/backend/.env.example`:
>
> ```env
> GROQ_API_KEY=PAINA_TÄHÄN_OMA_AVAIN
> GROQ_MODEL_ID=openai/gpt-oss-120b
> ```

Aseta koneelle oikea `GROQ_API_KEY`.

---

**2. Luo Groq-LLM-client backendille**

Cursor-ohje:

> Luo tiedosto `apps/backend/llm/groq_client.py` seuraavalla sisällöllä:
>
> ```python
> import os
> from typing import List
> from groq import Groq
>
> GROQ_MODEL_ID = os.getenv("GROQ_MODEL_ID", "openai/gpt-oss-120b")
>
> _client = Groq(api_key=os.environ["GROQ_API_KEY"])
>
> def build_context_block(chunks: List[dict]) -> str:
>     parts = []
>     for i, ch in enumerate(chunks, 1):
>         header = f"[LÄHDE {i}: {ch.get('doc_id')} §{ch.get('pykala_nro')} sivu {ch.get('sivu')}]"
>         parts.append(header + "\n" + ch["chunk_text"])
>     return "\n\n".join(parts)
>
> def ask_groq(system_prompt: str, question: str, chunks: List[dict], max_tokens: int = 700) -> str:
>     context = build_context_block(chunks)
>     user_content = (
>         f"Kysymys:\n{question}\n\n"
>         f"Hyödynnä alla olevaa kontekstia vastatessasi.\n"
>         f"Vastaa suomeksi, viittaa pykäliin ja päivämääriin.\n\n"
>         f"{context}"
>     )
>     resp = _client.chat.completions.create(
>         model=GROQ_MODEL_ID,
>         messages=[
>             {"role": "system", "content": system_prompt},
>             {"role": "user", "content": user_content},
>         ],
>         temperature=0.2,
>         max_tokens=max_tokens,
>     )
>     return resp.choices[0].message.content
> ```

---

**3. Kytke Groq LapuaQueryAgentiin**

Cursor-ohje:

> Avaa `packages/agents/query_agent.py` ja muuta `LapuaQueryAgent.answer()` käyttämään uutta Groq-clientiä:
>
> ```python
> from apps.backend.llm.groq_client import ask_groq
>
> class LapuaQueryAgent:
>     ...
>     def answer(self, plan: QueryPlan, chunks: list[dict]) -> dict:
>         system_prompt = (
>             "Olet Lapuan kaupungin asiakirjoihin erikoistunut assistentti. "
>             "Vastaat suomeksi, täsmällisesti ja viittaat aina päätöksen "
>             "tehneeseen toimielimeen, päivämäärään ja pykälänumeroon. "
>             "Älä keksi asioita, jos konteksti ei riitä."
>         )
>         answer_text = ask_groq(system_prompt, plan.question, chunks)
>         return {
>             "answer": answer_text,
>             "used_chunks": chunks,
>             "model": "openai/gpt-oss-120b",
>             "strategy": plan.strategy,
>         }
> ```

---

**4. Päivitä FastAPI-endpoint käyttämään agentin vastausta**

Cursor-ohje:

> Avaa `apps/backend/main.py` ja varmista, että `/query` kutsuu agenttia:
>
> ```python
> @app.post("/query")
> async def query(req: QueryRequest):
>     agent = LapuaQueryAgent()
>     plan = agent.plan(req.question, req.filters, mode=req.mode)
>     chunks = agent.retrieve(plan)
>     result = agent.answer(plan, chunks)
>     return result
> ```

---

**5. Testaa Groq-integraatio**

1. Aseta `GROQ_API_KEY` ja (tarvittaessa) `GROQ_MODEL_ID=openai/gpt-oss-120b`.
2. Käynnistä backend: `uvicorn apps.backend.main:app --reload`.
3. Testaa esim. `curl`/HTTP-clientilla kysymys “Simpsiönvuori Oy takaus” ja varmista, että vastauksen JSONissa `model` on `openai/gpt-oss-120b`.

---

### Tarkistuslista

**Fakta**

* Groq-mallin tunnus on **`openai/gpt-oss-120b`** Groq Cloudissa.
* Groq Python -clienti on `groq`-kirjasto, ja `chat.completions.create(...)`-rajapinta on OpenAI-yhteensopiva.

**Logiikka**

* RAG-pipeline (Docling → Qdrant → hybrid_search) ei muutu; vain `answer()`-vaihe vaihtaa LLM-providerin.
* Ympäristömuuttujat (`GROQ_API_KEY`, `GROQ_MODEL_ID`) mahdollistavat myöhemmin mallin vaihdon ilman koodimuutoksia.

**Kieli**

* Ohjeistus on suomeksi, Cursor-komennot ovat suoraan copy–paste-kelpoisia.
* Promptissa vaaditaan suomenkielinen, lähteisiin viittaava vastaus, mikä sopii Lapuan Kaupunki RAG -käyttöön.
